{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "612ca5be-bd88-4e7e-848f-7a3ab5e5b1a4",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow.keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m layers, models\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow.keras'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88387606-ac95-4a1d-82a9-fa8b3f42ead7",
   "metadata": {},
   "source": [
    "# Load and Preprocess the Data\n",
    "- The main issues with the data set right now is the non-uniform sizes and the values aren't normalized. So let's deal with both of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67b604d7-e024-449b-9695-a9c53af68ef8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "20000\n",
      "21000\n",
      "22000\n",
      "23000\n",
      "24000\n",
      "25000\n",
      "26000\n",
      "27000\n",
      "28000\n",
      "29000\n",
      "30000\n",
      "31000\n",
      "32000\n",
      "33000\n",
      "34000\n",
      "35000\n",
      "36000\n",
      "37000\n",
      "38000\n",
      "39000\n",
      "40000\n",
      "41000\n",
      "42000\n",
      "43000\n",
      "44000\n",
      "45000\n",
      "46000\n",
      "47000\n",
      "48000\n",
      "49000\n",
      "50000\n",
      "51000\n",
      "52000\n",
      "53000\n",
      "54000\n",
      "55000\n",
      "56000\n",
      "57000\n",
      "58000\n",
      "59000\n",
      "60000\n",
      "61000\n",
      "62000\n",
      "63000\n",
      "64000\n",
      "65000\n",
      "66000\n",
      "67000\n",
      "68000\n",
      "69000\n",
      "70000\n",
      "71000\n",
      "72000\n",
      "73000\n",
      "74000\n",
      "75000\n",
      "76000\n",
      "77000\n",
      "78000\n",
      "79000\n",
      "80000\n",
      "81000\n",
      "82000\n",
      "83000\n",
      "84000\n",
      "85000\n",
      "86000\n",
      "87000\n",
      "88000\n",
      "89000\n",
      "90000\n",
      "91000\n",
      "92000\n",
      "93000\n",
      "94000\n",
      "95000\n",
      "96000\n",
      "97000\n",
      "98000\n",
      "99000\n",
      "100000\n",
      "101000\n",
      "102000\n",
      "103000\n",
      "104000\n",
      "105000\n",
      "106000\n",
      "107000\n",
      "108000\n",
      "109000\n",
      "110000\n",
      "111000\n",
      "112000\n",
      "113000\n",
      "114000\n",
      "115000\n",
      "116000\n",
      "117000\n",
      "118000\n",
      "119000\n",
      "120000\n",
      "121000\n",
      "122000\n",
      "123000\n",
      "124000\n",
      "125000\n",
      "126000\n",
      "127000\n",
      "128000\n",
      "129000\n",
      "130000\n",
      "131000\n",
      "132000\n",
      "133000\n",
      "134000\n",
      "135000\n",
      "136000\n",
      "137000\n",
      "138000\n",
      "139000\n",
      "140000\n",
      "141000\n",
      "142000\n",
      "143000\n",
      "144000\n",
      "145000\n",
      "146000\n",
      "147000\n",
      "148000\n",
      "149000\n",
      "150000\n",
      "151000\n",
      "152000\n",
      "153000\n",
      "154000\n",
      "155000\n",
      "156000\n",
      "157000\n",
      "158000\n",
      "159000\n",
      "160000\n",
      "161000\n",
      "162000\n",
      "163000\n",
      "164000\n",
      "165000\n",
      "166000\n",
      "167000\n",
      "168000\n",
      "169000\n",
      "170000\n",
      "171000\n",
      "172000\n",
      "173000\n",
      "174000\n",
      "175000\n",
      "176000\n",
      "177000\n",
      "178000\n",
      "179000\n",
      "180000\n",
      "181000\n",
      "182000\n",
      "183000\n",
      "184000\n",
      "185000\n",
      "186000\n",
      "187000\n",
      "188000\n",
      "189000\n",
      "190000\n",
      "191000\n",
      "192000\n",
      "193000\n",
      "194000\n",
      "195000\n",
      "196000\n",
      "197000\n",
      "198000\n",
      "199000\n",
      "200000\n",
      "201000\n",
      "202000\n",
      "203000\n",
      "204000\n",
      "205000\n",
      "206000\n",
      "207000\n",
      "208000\n",
      "209000\n",
      "210000\n",
      "211000\n",
      "212000\n",
      "213000\n",
      "214000\n",
      "215000\n",
      "216000\n",
      "217000\n",
      "218000\n",
      "219000\n",
      "220000\n",
      "221000\n",
      "222000\n",
      "223000\n",
      "224000\n",
      "225000\n",
      "226000\n",
      "227000\n",
      "228000\n",
      "229000\n",
      "230000\n",
      "231000\n",
      "232000\n",
      "233000\n",
      "234000\n",
      "235000\n",
      "236000\n",
      "237000\n",
      "238000\n",
      "239000\n",
      "240000\n",
      "241000\n",
      "242000\n",
      "243000\n",
      "244000\n",
      "245000\n",
      "246000\n",
      "247000\n",
      "248000\n",
      "249000\n",
      "250000\n",
      "251000\n",
      "252000\n",
      "253000\n",
      "254000\n",
      "255000\n",
      "256000\n",
      "257000\n",
      "258000\n",
      "259000\n",
      "260000\n",
      "261000\n",
      "262000\n",
      "263000\n",
      "264000\n",
      "265000\n",
      "266000\n",
      "267000\n",
      "268000\n",
      "269000\n",
      "270000\n",
      "271000\n",
      "272000\n",
      "273000\n",
      "274000\n",
      "275000\n",
      "276000\n",
      "277000\n",
      "278000\n",
      "279000\n",
      "280000\n",
      "281000\n",
      "282000\n",
      "283000\n",
      "284000\n",
      "285000\n",
      "286000\n",
      "287000\n",
      "288000\n",
      "289000\n",
      "290000\n",
      "291000\n",
      "292000\n",
      "293000\n",
      "294000\n",
      "295000\n",
      "296000\n",
      "297000\n",
      "298000\n",
      "299000\n",
      "300000\n",
      "301000\n",
      "302000\n",
      "303000\n",
      "304000\n",
      "305000\n",
      "306000\n",
      "307000\n",
      "308000\n",
      "309000\n",
      "310000\n",
      "311000\n",
      "312000\n",
      "313000\n",
      "314000\n",
      "315000\n",
      "316000\n",
      "317000\n",
      "318000\n",
      "319000\n",
      "320000\n",
      "321000\n",
      "322000\n",
      "323000\n",
      "324000\n",
      "325000\n",
      "326000\n",
      "327000\n",
      "328000\n",
      "329000\n",
      "330000\n",
      "331000\n",
      "332000\n",
      "333000\n",
      "334000\n",
      "335000\n",
      "336000\n",
      "337000\n",
      "338000\n",
      "339000\n",
      "340000\n",
      "341000\n",
      "342000\n",
      "343000\n",
      "344000\n",
      "345000\n",
      "346000\n",
      "347000\n",
      "348000\n",
      "349000\n",
      "350000\n",
      "351000\n",
      "352000\n",
      "353000\n",
      "354000\n",
      "355000\n",
      "356000\n",
      "357000\n",
      "358000\n",
      "359000\n",
      "360000\n",
      "361000\n",
      "362000\n",
      "363000\n",
      "364000\n",
      "365000\n",
      "366000\n",
      "367000\n",
      "368000\n",
      "369000\n",
      "370000\n",
      "371000\n",
      "372000\n"
     ]
    }
   ],
   "source": [
    "# NOTE! This code took around 10-15 minutes to load for me\n",
    "\n",
    "path = 'alphabets_dataset/alphabet_images'\n",
    "sorted_filenames = sorted(os.listdir(path))\n",
    "image_list = []\n",
    "reference = 0\n",
    "for image in sorted_filenames:\n",
    "    img = Image.open(path + '/' + image)\n",
    "    img = img.resize((28, 28))\n",
    "    img = np.array(img)\n",
    "    image_list.append(img)\n",
    "    if reference % 1000 == 0:\n",
    "        print(reference)\n",
    "    reference += 1\n",
    "\n",
    "image_list = np.array(image_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "756b0d8d-f8c1-4b87-aea3-261528da772e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "[ 0  0  0 ... 11 11 11]\n",
      "Images' shape: (372451, 28, 28)\n",
      "Labels' shape: (372451,)\n"
     ]
    }
   ],
   "source": [
    "# Reading just the labels\n",
    "labels_df = pd.read_csv('alphabets_dataset/alphabet_labels.csv')\n",
    "\n",
    "labels_df = labels_df.sort_values(by='file')\n",
    "labels = labels_df['label'].values\n",
    "print(type(labels))\n",
    "\n",
    "# Encode labels (will do if required)\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "labels_encoded = label_encoder.fit_transform(labels)\n",
    "labels = labels_encoded\n",
    "\n",
    "print(f\"Images' shape: {image_list.shape}\")\n",
    "print(f\"Labels' shape: {labels.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5d32caae-2490-4879-8ee6-39b83d63e410",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   5  24  26   8\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   1  51 175 180  64\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0  10 130 239 231 108\n",
      "    4   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   2  43 206 254 233 120\n",
      "    6   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0  28 143 246 255 240 151\n",
      "   10   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0  18 105 231 252 250 244 172\n",
      "   13   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   2  68 200 254 206 204 235 177\n",
      "   13   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0  18 124 235 233 114 154 224 168\n",
      "   12   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0  63 199 240 152  30 136 213 124\n",
      "    3   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0  12 121 234 236 115  42 148 216 128\n",
      "   27  22  16   3   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   6  74 205 252 252 229 195 216 238 203\n",
      "  162 161 121  24   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  21 154 243 255 255 244 241 245 253 250\n",
      "  239 233 181  36   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   4  55 230 254 240 203 137 147 187 241 239\n",
      "  168 121  72  13   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0  12  94 248 241 142  46  16  19  94 216 187\n",
      "   34  13   5   1   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0  43 191 255 209  60   0   0   0  78 213 182\n",
      "   12   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0  13 107 238 255 157  29   0   0   0  72 216 209\n",
      "   26   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0  33 176 252 228  85  11   0   0   0  40 193 223\n",
      "   43   1   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0  22 125 187 125  23   1   0   0   0  25 160 228\n",
      "  106  10   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   3  17  27  15   1   0   0   0   0  18 132 232\n",
      "  180  22   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   7  91 232\n",
      "  225  34   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   4  71 189\n",
      "  146  19   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   1  11  28\n",
      "   18   2   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(image_list[0])\n",
    "plt.imshow(image_list_copy[0], cmap='gray')\n",
    "plt.title('First Image')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3b4f8a37-0e46-41e4-bba9-c18c316c4190",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape: (297960, 28, 28), Labels shape: (297960,)\n",
      "Test data shape: (74491, 28, 28), Labels shape: (74491,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(image_list, labels, test_size=0.2, random_state=45)\n",
    "X_train = X_train.astype('float32') / 255.0\n",
    "X_test = X_test.astype('float32') / 255.0\n",
    "\n",
    "print(f\"Train data shape: {X_train.shape}, Labels shape: {y_train.shape}\")\n",
    "print(f\"Test data shape: {X_test.shape}, Labels shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5927db23-da96-472c-a5c3-dca9bb2097fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m596/596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m457s\u001b[0m 759ms/step - accuracy: 0.7785 - loss: 0.7965 - val_accuracy: 0.9719 - val_loss: 0.1052\n",
      "Epoch 2/10\n",
      "\u001b[1m596/596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m392s\u001b[0m 657ms/step - accuracy: 0.9731 - loss: 0.0964 - val_accuracy: 0.9801 - val_loss: 0.0743\n",
      "Epoch 3/10\n",
      "\u001b[1m596/596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m387s\u001b[0m 643ms/step - accuracy: 0.9809 - loss: 0.0685 - val_accuracy: 0.9831 - val_loss: 0.0629\n",
      "Epoch 4/10\n",
      "\u001b[1m596/596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m380s\u001b[0m 637ms/step - accuracy: 0.9853 - loss: 0.0543 - val_accuracy: 0.9836 - val_loss: 0.0585\n",
      "Epoch 5/10\n",
      "\u001b[1m596/596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m316s\u001b[0m 526ms/step - accuracy: 0.9876 - loss: 0.0447 - val_accuracy: 0.9857 - val_loss: 0.0524\n",
      "Epoch 6/10\n",
      "\u001b[1m596/596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m267s\u001b[0m 448ms/step - accuracy: 0.9888 - loss: 0.0393 - val_accuracy: 0.9879 - val_loss: 0.0449\n",
      "Epoch 7/10\n",
      "\u001b[1m596/596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m348s\u001b[0m 583ms/step - accuracy: 0.9905 - loss: 0.0334 - val_accuracy: 0.9889 - val_loss: 0.0407\n",
      "Epoch 8/10\n",
      "\u001b[1m596/596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m254s\u001b[0m 426ms/step - accuracy: 0.9914 - loss: 0.0298 - val_accuracy: 0.9888 - val_loss: 0.0408\n",
      "Epoch 9/10\n",
      "\u001b[1m596/596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m246s\u001b[0m 412ms/step - accuracy: 0.9918 - loss: 0.0273 - val_accuracy: 0.9878 - val_loss: 0.0441\n",
      "Epoch 10/10\n",
      "\u001b[1m596/596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1332s\u001b[0m 2s/step - accuracy: 0.9930 - loss: 0.0225 - val_accuracy: 0.9904 - val_loss: 0.0354\n",
      "2328/2328 - 18s - 8ms/step - accuracy: 0.9904 - loss: 0.0354\n",
      "Test accuracy: 0.9903612732887268\n"
     ]
    }
   ],
   "source": [
    "# Define CNN model architecture\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.InputLayer(shape = (28, 28, 1)),\n",
    "    tf.keras.layers.Conv2D(32, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(26, activation='softmax')  # 26 classes (assuming one for each alphabet)\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=10, batch_size=500, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test, verbose=2)\n",
    "print(f'Test accuracy: {test_acc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15937834-b6fd-460f-af09-c7c0a313183a",
   "metadata": {},
   "source": [
    "## Ideas to implement for the classification model\n",
    "- Use a dummy regressor for the alphabet classification model (https://www.codecademy.com/paths/build-deep-learning-models-with-tensorflow/tracks/dlsp-getting-started-with-tensorflow/modules/dlsp-implementing-neural-networks/lessons/hyperparameter-tuning-neural/exercises/baselines-neural-network)\n",
    "- Regularization (dropout) (https://www.codecademy.com/paths/build-deep-learning-models-with-tensorflow/tracks/dlsp-getting-started-with-tensorflow/modules/dlsp-implementing-neural-networks/lessons/hyperparameter-tuning-neural/exercises/regularization-dropout)\n",
    "- Early Stopping for no. of epochs (https://www.codecademy.com/paths/build-deep-learning-models-with-tensorflow/tracks/dlsp-getting-started-with-tensorflow/modules/dlsp-implementing-neural-networks/lessons/hyperparameter-tuning-neural/exercises/tuning-epochs-early-stopping)\n",
    "- Grid Search or Randomized Search (batch size, (learning_rate?), (and maybe neuron shape?)\n",
    "- Some graphs to show learning and error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5326a612-3921-4ad6-bdf8-f3f5d0f1bd3c",
   "metadata": {},
   "source": [
    "# Inputting Text From Image\n",
    "The given images are in a grid format. If we can assume each alphabet to be 28x28 grid, the process simplifies to splitting the grid into 28x28 grids, running the model on the grid, and appending it to a list or something. \n",
    "\n",
    "Then, the main issue is how to tackle the spaces, one idea is to just look for a blank grid. The other is to change the last dense layer to have 27 possible outputs instead of the original 26. Considering the confidence level of the model, when inputted a blank grid, they will not have much confidence and assign the 27th option to some minimal probability. We can just check for that.\n",
    "\n",
    "Pseudocode:\n",
    "1. Take all target images input and store it in a np array (like we did for the training images earlier)\n",
    "2. For each image, cut it up using a library (a good option looks like cv2 or image_slicer) and store them in a list. (image_slicer looks more intuitive to use, but it stores images as an Image object, that need to be converted to "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
